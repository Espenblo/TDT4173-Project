{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Single Model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Espenblo/TDT4173-Project/blob/main/Single_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Z0oHAk9yr7"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go-10uXwVmf7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import pandas_datareader as web\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error              \n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import MAPE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXLNUhZYnNdx"
      },
      "source": [
        "# **Import Data From Yahoo and create dataframe for first stock**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWRqmTp_mBjG"
      },
      "source": [
        "# Fetch dataset\n",
        "#TODO: Pick the stock you want to predict\n",
        "#df = web.DataReader(\"BPM\", data_source=\"yahoo\", start=\"1997-01-01\", end=\"2020-01-01\") #BP/LUKOY/XOM\n",
        "#df = web.DataReader(\"INTC\", data_source=\"yahoo\", start=\"1990-01-01\", end=\"2020-01-01\") #INTC/IBM/MSFT\n",
        "df = web.DataReader(\"F\", data_source=\"yahoo\", start=\"2004-09-26\", end=\"2020-01-01\") #F/TM/TTM\n",
        "\n",
        "#Dropping a colum from dataframe\n",
        "df = df.drop(['Adj Close'], axis=1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBvFuCsZ-Qm9"
      },
      "source": [
        "# **Data info and plot for first stock**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWTx4Sm3-bni"
      },
      "source": [
        "# Display dataset info\n",
        "df.info()\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4jQM2ssm0-i"
      },
      "source": [
        "# Illustrate closing price data\n",
        "df['Close'].plot(figsize=(12,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5EA7kazm9g0"
      },
      "source": [
        "# Illustrate volume data\n",
        "df['Volume'].plot(figsize=(12,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFkKjYUlnY9M"
      },
      "source": [
        "# **Scale and split data for first stock**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL_IfdQXXXms"
      },
      "source": [
        "# Round off the data to two decimals, to remove excess noice\n",
        "df = df.round(2)\n",
        "\n",
        "# Create the scalers\n",
        "scaler_high = MinMaxScaler()\n",
        "scaler_low = MinMaxScaler()\n",
        "scaler_open = MinMaxScaler()\n",
        "scaler_close = MinMaxScaler()\n",
        "scaler_volume = MinMaxScaler()\n",
        "\n",
        "# Reshape\n",
        "high_value = df['High'].values\n",
        "high_value = high_value.reshape(len(high_value), 1)\n",
        "\n",
        "low_value = df['Low'].values\n",
        "low_value = low_value.reshape(len(low_value), 1)\n",
        "\n",
        "open_value = df['Open'].values\n",
        "open_value = open_value.reshape(len(open_value), 1)\n",
        "\n",
        "close_value = df['Close'].values\n",
        "close_value = close_value.reshape(len(close_value), 1)\n",
        "\n",
        "volume_value = df['Volume'].values\n",
        "volume_value = volume_value.reshape(len(volume_value), 1)\n",
        "\n",
        "# Fit scaler to training data\n",
        "scaler_high.fit(high_value)\n",
        "scaler_low.fit(low_value)\n",
        "scaler_open.fit(open_value)\n",
        "scaler_close.fit(close_value)\n",
        "scaler_volume.fit(volume_value)\n",
        "\n",
        "# Perform scaler transformation\n",
        "scaled_high = scaler_high.transform(high_value)\n",
        "scaled_low = scaler_low.transform(low_value)\n",
        "scaled_open = scaler_open.transform(open_value)\n",
        "scaled_close = scaler_close.transform(close_value)\n",
        "scaled_volume = scaler_volume.transform(volume_value)\n",
        "\n",
        "# Combine data to dataframe \n",
        "df['scaled_high'] = scaled_high\n",
        "df['scaled_low'] = scaled_low\n",
        "df['scaled_open'] = scaled_open\n",
        "df['scaled_close'] = scaled_close\n",
        "df['scaled_volume'] = scaled_volume\n",
        "\n",
        "# Create final dataframe with scaled values\n",
        "normalizedData = df[[\"scaled_high\", \"scaled_low\", \"scaled_open\", \"scaled_close\", \"scaled_volume\"]].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdu_CB1OUkif"
      },
      "source": [
        "# Create an input set and a test set. View the \"Split\" section of the paper to get an illustration\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in range (0,len(normalizedData)-65):\n",
        "  x.append(normalizedData[i:i+60])\n",
        "  y.append(normalizedData[i+60:i+65, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ILVSddq7D5"
      },
      "source": [
        "# Remove values which can't be predicted 5 days ahead\n",
        "cropped_df = df.tail(-60).head(-5)\n",
        "\n",
        "# Split into training set (80%) and test set (20%)\n",
        "split_off_index = round(len(cropped_df)*0.80)\n",
        "\n",
        "# Split into test set and training set (for evaluation)\n",
        "train_eval = cropped_df.iloc[:split_off_index]\n",
        "test_eval = cropped_df.iloc[split_off_index:]\n",
        "\n",
        "# Split into training set and test set (for training)\n",
        "train_x = x[:split_off_index]\n",
        "train_y = y[:split_off_index]\n",
        "\n",
        "test_x = x[split_off_index:]\n",
        "test_y = y[split_off_index:]\n",
        "\n",
        "# Convert to numpy array\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n",
        "\n",
        "test_x = np.array(test_x)\n",
        "test_y = np.array(test_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64IFxgd3-RDA"
      },
      "source": [
        "# Plot Scaled prices\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Scaled closing price')\n",
        "plt.xlabel('Time', fontsize=18)\n",
        "plt.ylabel('Closing price scaled', fontsize=18)\n",
        "plt.plot(normalizedData[:,3])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "df['Close'].plot(figsize=(12,8))\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "plt.xlabel('Time', fontsize=18)\n",
        "plt.ylabel('Closing price', fontsize=18)\n",
        "plt.plot(df.Close)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "plt.xlabel('Time', fontsize=18)\n",
        "plt.ylabel('volume scaled',fontsize =18)\n",
        "plt.plot(normalizedData[:,4])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "plt.xlabel('Time', fontsize=18)\n",
        "plt.ylabel('volume 1e7',fontsize =18)\n",
        "plt.plot(df.Volume)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFEIBImxcl0p"
      },
      "source": [
        "# Display dimensionality on training and test sets\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIIrKhs_F3qH"
      },
      "source": [
        "# **Create the models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmHPASI8F6DY"
      },
      "source": [
        "# Define the model\n",
        "model_1_layer = Sequential()\n",
        "model_3_layer = Sequential()\n",
        "\n",
        "# Add LSTM layer for 1 layer model\n",
        "model_1_layer.add(LSTM(128, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
        "model_1_layer.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "# Add LSTM layer for 3 layer model\n",
        "model_3_layer.add(LSTM(128, return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
        "model_3_layer.add(Dropout(0.2))\n",
        "\n",
        "model_3_layer.add(LSTM(64, return_sequences=True))\n",
        "model_3_layer.add(Dropout(0.2))\n",
        "\n",
        "model_3_layer.add(LSTM(32))\n",
        "model_3_layer.add(Dropout(0.2))\n",
        "\n",
        "# Final Prediction - 5 outputs, 5 neurons\n",
        "model_1_layer.add(Dense(5))\n",
        "model_3_layer.add(Dense(5))\n",
        "\n",
        "#Compile\n",
        "model_1_layer.compile(optimizer='adam', loss='mse')\n",
        "model_3_layer.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuyz59S-J8l_"
      },
      "source": [
        "model_1_layer.summary()\n",
        "model_3_layer.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLBMXsDHKAf1"
      },
      "source": [
        "# **Train individual models on the first stock**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXLu3nmzKGax"
      },
      "source": [
        "# Early stop\n",
        "early_stop = EarlyStopping(monitor='val_loss',patience=20)\n",
        "\n",
        "# Fit model\n",
        "def fit_models():\n",
        "  model_1_layer.fit(train_x, train_y, epochs=200, validation_data=(test_x, test_y), callbacks=[early_stop], batch_size=32)\n",
        "  model_3_layer.fit(train_x, train_y, epochs=200, validation_data=(test_x, test_y), callbacks=[early_stop,], batch_size=32)\n",
        "\n",
        "  losses_1_layer = pd.DataFrame(model_1_layer.history.history)\n",
        "  losses_1_layer.plot()\n",
        "  losses_3_layer = pd.DataFrame(model_3_layer.history.history)\n",
        "  losses_3_layer.plot()\n",
        "fit_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAF5KMoPKLag"
      },
      "source": [
        "# **Let each model make predictions on the test data for the first stock**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKX9yp8dK4Lb"
      },
      "source": [
        "#Create loop and predict\n",
        "n_features = 5\n",
        "test_predictions_1_layer = []\n",
        "test_predictions_3_layer = []\n",
        "\n",
        "\n",
        "for i in range(0,len(test_x)):\n",
        "    test_batch = test_x[i]\n",
        "    reshaped_batch = test_batch.reshape((1, 60, n_features))\n",
        "    \n",
        "    # get prediction 1-5 time stamps ahead ([0] is for grabbing just the number instead of [array])\n",
        "    current_pred_1_layer = model_1_layer.predict(reshaped_batch)[0]\n",
        "    current_pred_3_layer = model_3_layer.predict(reshaped_batch)[0]\n",
        "    \n",
        "    # store prediction\n",
        "    test_predictions_1_layer.append(current_pred_1_layer)\n",
        "    test_predictions_3_layer.append(current_pred_3_layer)  \n",
        "\n",
        "# Inverse transform\n",
        "true_predictions_1_layer = scaler_close.inverse_transform(test_predictions_1_layer)\n",
        "true_predictions_3_layer = scaler_close.inverse_transform(test_predictions_3_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdPyewfQXeh5"
      },
      "source": [
        "# **Compare the actual closing price to the predicted closing price for each model and calculate evaluation metrics (MAPE and RMSE) for each of the 5 predicted days**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMuycbL1W20j"
      },
      "source": [
        "result_array_1_layer = []\n",
        "result_array_3_layer = []\n",
        "date_array = []\n",
        "for i in range(5, len(test_eval)):\n",
        "  temp_pred_array_1_layer = []\n",
        "  temp_pred_array_3_layer = []\n",
        "  date_array.append(test_eval.index[i])\n",
        "  temp_pred_array_1_layer.append(test_eval['Close'].values[i])\n",
        "  temp_pred_array_3_layer.append(test_eval['Close'].values[i])\n",
        "  for j in range(1,6):\n",
        "    temp_pred_array_1_layer.append(true_predictions_1_layer[i-j][j-1])\n",
        "    temp_pred_array_3_layer.append(true_predictions_3_layer[i-j][j-1])\n",
        "  result_array_1_layer.append(temp_pred_array_1_layer)\n",
        "  result_array_3_layer.append(temp_pred_array_3_layer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFpmrmnxcN_E"
      },
      "source": [
        "# Create result dataframes\n",
        "res_df_1_layer = pd.DataFrame(result_array_1_layer, columns = ['Actual price','1 day prediction',\"2 day prediction\", \"3 day prediction\", \"4 day prediction\", \"5 day prediction\"])\n",
        "res_df_1_layer.index = date_array\n",
        "res_df_3_layer = pd.DataFrame(result_array_3_layer, columns = ['Actual price','1 day prediction',\"2 day prediction\", \"3 day prediction\", \"4 day prediction\", \"5 day prediction\"])\n",
        "res_df_3_layer.index = date_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT_SMhxb_GgL"
      },
      "source": [
        "# Calculate MAPE and RMSE for 1-5 day predictions\n",
        "evaluation_array_1_layer = []\n",
        "evaluation_array_3_layer = []\n",
        "for i in range (1,6):\n",
        "  mape_err_1_layer = MAPE(res_df_1_layer.iloc[:,0], res_df_1_layer.iloc[:,i]).numpy().round(4)\n",
        "  mape_err_3_layer = MAPE(res_df_3_layer.iloc[:,0], res_df_3_layer.iloc[:,i]).numpy().round(4)\n",
        "  rmse_err_1_layer = np.sqrt(mean_squared_error(res_df_1_layer.iloc[:,0], res_df_1_layer.iloc[:,i])).round(4)\n",
        "  rmse_err_3_layer = np.sqrt(mean_squared_error(res_df_3_layer.iloc[:,0], res_df_3_layer.iloc[:,i])).round(4)\n",
        "  evaluation_array_1_layer.append([mape_err_1_layer, rmse_err_1_layer])\n",
        "  evaluation_array_3_layer.append([mape_err_3_layer, rmse_err_3_layer])\n",
        "\n",
        "error_df_1_layer = pd.DataFrame(evaluation_array_1_layer, columns = ['MAPE','RMSE'])\n",
        "error_df_1_layer.index = ['1 day prediction',\"2 day prediction\", \"3 day prediction\", \"4 day prediction\", \"5 day prediction\"]\n",
        "print(\"Single model 1 layer network\")\n",
        "print(error_df_1_layer)\n",
        "print()\n",
        "error_df_3_layer = pd.DataFrame(evaluation_array_3_layer, columns = ['MAPE','RMSE'])\n",
        "error_df_3_layer.index = ['1 day prediction',\"2 day prediction\", \"3 day prediction\", \"4 day prediction\", \"5 day prediction\"]\n",
        "print(\"Single model 3 layer network\")\n",
        "print(error_df_3_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q93-KCPJcYP3"
      },
      "source": [
        "# Plot actual closing price against predicted closing price for 1 layer model\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Actual vs predicted closing price for single 1 layer model')\n",
        "plt.xlabel('Date', fontsize=18)\n",
        "plt.ylabel('Close price USD ($)', fontsize=18)\n",
        "plt.plot(res_df_1_layer['Actual price'])\n",
        "plt.plot(res_df_1_layer['1 day prediction'])\n",
        "plt.plot(res_df_1_layer['5 day prediction'])\n",
        "plt.xlim(datetime.date(2017,1,1), datetime.date(2020,1,1))\n",
        "plt.legend(['Actual price', '1 day prediction', \"5 day prediction\"], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Plot actual closing price against predicted closing price for 3 layer model\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Actual vs predicted closing price for single 3 layer model')\n",
        "plt.xlabel('Date', fontsize=18)\n",
        "plt.ylabel('Close price USD ($)', fontsize=18)\n",
        "plt.plot(res_df_3_layer['Actual price'])\n",
        "plt.plot(res_df_3_layer['1 day prediction'])\n",
        "plt.plot(res_df_3_layer['5 day prediction'])\n",
        "plt.xlim(datetime.date(2017,1,1), datetime.date(2020,1,1))\n",
        "plt.legend(['Actual price', '1 day prediction', \"5 day prediction\"], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_-FYohqW1Jg"
      },
      "source": [
        "# **Save single models to google drive (optional)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFWreeQCW0pR"
      },
      "source": [
        "#Save and load model imports\n",
        "#!pip install -U -q PyDrive\n",
        "#from pydrive.auth import GoogleAuth\n",
        "#from pydrive.drive import GoogleDrive \n",
        "#from google.colab import auth \n",
        "#from oauth2client.client import GoogleCredentials\n",
        " \n",
        "#auth.authenticate_user()\n",
        "#gauth = GoogleAuth()\n",
        "#gauth.credentials = GoogleCredentials.get_application_default()                       \n",
        "#drive = GoogleDrive(gauth)\n",
        "\n",
        "#Change save names \n",
        "# Save models\n",
        "#model_1_layer.save('Single-1-layer-model-F.h5')\n",
        "#model_1_layer_file = drive.CreateFile({'title' : 'Single-1-layer-model-F.h5'})                       \n",
        "#model_1_layer_file.SetContentFile('Single-1-layer-model-F.h5')                       \n",
        "#model_1_layer_file.Upload()\n",
        "\n",
        "#model_3_layer.save('Single-3-layer-model-F.h5')\n",
        "#model_3_layer_file = drive.CreateFile({'title' : 'Single-3-layer-model-F.h5'})                       \n",
        "#model_3_layer_file.SetContentFile('Single-3-layer-model-F.h5')                       \n",
        "#model_3_layer_file.Upload()\n",
        " \n",
        "#Get drive ID - MAKE SURE TO SAVE THE ID SOMEWHERE\n",
        "#print(\"Single 1 layer model:\")              \n",
        "#print(drive.CreateFile({'id': model_1_layer_file.get('id')}))\n",
        "#print(\"Single 3 layer model:\")  \n",
        "#print(drive.CreateFile({'id': model_3_layer_file.get('id')}))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pySEaiwYoCZa"
      },
      "source": [
        "# **Load model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfkZjQ43oIc6"
      },
      "source": [
        "#model = load_model(\"Combined-3-layer-model-F.h5\")\n",
        "#model = model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}